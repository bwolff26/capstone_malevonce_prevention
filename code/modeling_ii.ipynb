{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18706f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d323ed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import regex as re\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, ComplementNB\n",
    "from sklearn.svm import SVC\n",
    "#Forests\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning);\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from random import choices\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "844f7115",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_base.pkl', 'rb') as f: #reading bytes\n",
    "    foo = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20733450",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/cleaned_02_ii.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "700ce705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Pipeline.get_params of Pipeline(steps=[('tvec',\n",
       "                 TfidfVectorizer(lowercase=False, max_df=0.95, min_df=0.0,\n",
       "                                 ngram_range=(1, 2))),\n",
       "                ('lr',\n",
       "                 LogisticRegression(class_weight={0: 0.8, 1: 3, 2: 6},\n",
       "                                    max_iter=3000,\n",
       "                                    multi_class='multinomial'))])>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5437d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_limit, upper_limit = .5, 2\n",
    "df['hate_rank'] = [2 if i > upper_limit\n",
    "                  else 1 if i > lower_limit else 0\n",
    "                  for i in df['hate_speech_score']]\n",
    "df['hate_rank'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6654cb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['cleaned_text']\n",
    "y = df['hate_rank']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = 3785, #To have a clean 200 2s in the test 3785 pref\n",
    "                                                    random_state = 26, #I like this number\n",
    "                                                    stratify=y) #Particularly important in this case with 3 targets\n",
    "\n",
    "# Hmm, as it stands right now y_test has mostly 0s... eh, I think htat's fine as it's like new data. So, cvs might be a little\n",
    "# scruff now with seeing thet sam thing,b ut eh, better htan nothing.... And, likely such is the inhernt issue of\n",
    "# using such methodologies..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fcbb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(X_train)\n",
    "data['hate_rank'] = y_train\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d222474",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = []\n",
    "for i in range(3):\n",
    "    data_names.append(f\"data_{i}\")\n",
    "data_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d03662",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data_names)):#Recall that we're still planning on doing replace=True even for 0.\n",
    "    data_names[i] = data[data['hate_rank']==i].sample(col_size, replace=True, random_state=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d3859f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat(data_names)\n",
    "print(data.shape)\n",
    "data.head()\n",
    "X_train = data['cleaned_text']\n",
    "y_train = data['hate_rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeacc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cvec_ex = CountVectorizer(stop_words='english')\n",
    "# X_train_cvec_ex = cvec_ex.fit_transform(X_train)\n",
    "# X_train_ex = pd.DataFrame(X_train_cvec_ex.todense(),\n",
    "#                           columns=cvec_ex.get_feature_names_out())\n",
    "\n",
    "# X_train_ex.sum().sort_values().tail(30).plot(kind=\"barh\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a0c8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial_model.named_steps['lr'].coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a656d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial_model.named_steps['tvec'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2995d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_recall(y, ypred, weight_list):\n",
    "    numer = 0\n",
    "    for i in range(len(weight_list)):\n",
    "        numer += weight_list[i] * recall_score(y, ypred, average=None)[i]\n",
    "    return numer/sum(weight_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfa5f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_weights={\n",
    "    0:.8\n",
    "    ,1:3\n",
    "    ,2:6\n",
    "}\n",
    "cw_values = list(current_weights.values())\n",
    "print(cw_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db28fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipe = Pipeline([\n",
    "#     ('tvec', TfidfVectorizer(lowercase=False))\n",
    "#     ,('lr', LogisticRegression(max_iter=3000, multi_class='multinomial', class_weight=current_weights))\n",
    "# ])\n",
    "# pipe_params = {'tvec__min_df' : [.0]\n",
    "#                ,'tvec__max_df' : [.95]\n",
    "#                ,'tvec__ngram_range' : [(1,2)]\n",
    "#                ,'tvec__max_features' : [None]\n",
    "#                ,'tvec__stop_words' : [None]\n",
    "#                }\n",
    "# scorer = make_scorer(custom_recall, weight_list=cw_values[1:]) #Note the 1: as we're now ignoring 0 for the scoring\n",
    "# gs = GridSearchCV(pipe\n",
    "#                   ,param_grid=pipe_params\n",
    "#                   ,cv=5\n",
    "#                   ,scoring=scorer\n",
    "#                  )\n",
    "\n",
    "# gs.fit(X_train, y_train)\n",
    "\n",
    "# print(gs.score(X_train, y_train), gs.score(X_test, y_test))\n",
    "# print(gs.best_score_)\n",
    "# print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f60171",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_model = gs.best_estimator_\n",
    "trial_model.fit(X_train,y_train)\n",
    "print(trial_model.score(X_train, y_train), trial_model.score(X_test, y_test))\n",
    "\n",
    "preds = trial_model.predict(X_test)\n",
    "preds_prob = trial_model.predict_proba(X_test)\n",
    "hyper_pred_comparer = pd.DataFrame({\n",
    "    'cleaned_text':X_test\n",
    "    ,'actual':y_test\n",
    "    ,'pred':preds\n",
    "    ,'p_0':preds_prob[:,0]\n",
    "    ,'p_1':preds_prob[:,1]\n",
    "    ,'p_2':preds_prob[:,2]\n",
    "})\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(trial_model,X_test,y_test, cmap='Greens');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31cebbc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m##0.6277581120943954\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m class_twos \u001b[38;5;241m=\u001b[39m confusion_matrix(y_test, preds)[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m      3\u001b[0m class_ones \u001b[38;5;241m=\u001b[39m confusion_matrix(y_test, preds)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      4\u001b[0m (cw_values[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m(class_ones[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;28msum\u001b[39m(class_ones))\u001b[38;5;241m+\u001b[39mcw_values[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m*\u001b[39m(class_twos[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;28msum\u001b[39m(class_twos)))\u001b[38;5;241m/\u001b[39m\u001b[38;5;28msum\u001b[39m(cw_values[\u001b[38;5;241m1\u001b[39m:])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "##0.6277581120943954\n",
    "class_twos = confusion_matrix(y_test, preds)[2]\n",
    "class_ones = confusion_matrix(y_test, preds)[1]\n",
    "(cw_values[1]*(class_ones[1]/sum(class_ones))+cw_values[2]*(class_twos[2]/sum(class_twos)))/sum(cw_values[1:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6aaece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
